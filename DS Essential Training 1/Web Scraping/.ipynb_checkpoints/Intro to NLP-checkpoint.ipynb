{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440c29f4",
   "metadata": {},
   "source": [
    "# Chapter 6 - Data Sourcing via Web\n",
    "\n",
    "## Segment 5 - Introduction to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5747235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdd5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"On Wednesday, the Association for Computing Machinery, the world’s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this year’s Turing Award for their work on neural networks. The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a683ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\imran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0594bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenising the text: \n",
      "\n",
      "['On Wednesday, the Association for Computing Machinery, the world’s largest society of computing professionals, announced that Hinton, LeCun and Bengio had won this year’s Turing Award for their work on neural networks.', 'The Turing Award, which was introduced in 1966, is often called the Nobel Prize of computing, and it includes a $1 million prize, which the three scientists will share.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokensiser\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tk = sent_tokenize(text)\n",
    "print(\"Sentence tokenising the text: \\n\")\n",
    "print(sent_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda97a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenising the text: \n",
      "\n",
      "['On', 'Wednesday', ',', 'the', 'Association', 'for', 'Computing', 'Machinery', ',', 'the', 'world', '’', 's', 'largest', 'society', 'of', 'computing', 'professionals', ',', 'announced', 'that', 'Hinton', ',', 'LeCun', 'and', 'Bengio', 'had', 'won', 'this', 'year', '’', 's', 'Turing', 'Award', 'for', 'their', 'work', 'on', 'neural', 'networks', '.', 'The', 'Turing', 'Award', ',', 'which', 'was', 'introduced', 'in', '1966', ',', 'is', 'often', 'called', 'the', 'Nobel', 'Prize', 'of', 'computing', ',', 'and', 'it', 'includes', 'a', '$', '1', 'million', 'prize', ',', 'which', 'the', 'three', 'scientists', 'will', 'share', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokensiser\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tk = word_tokenize(text)\n",
    "print(\"Word tokenising the text: \\n\")\n",
    "print(word_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484f72e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\imran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7f3609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words in English language are: \n",
      "\n",
      "{'into', 'now', 'hers', 'she', 'was', 'more', 'yourselves', 'we', \"won't\", 'm', \"that'll\", 'out', 'while', 's', 'and', 'these', 'some', 'had', 'during', 'aren', 'doing', 'am', \"don't\", 'herself', 'having', 'ain', 'in', \"you're\", 'when', 'yours', 'there', 'didn', \"shouldn't\", 'he', 'because', 'or', 'shouldn', 'ours', 'me', 'did', 'have', \"you'd\", \"mustn't\", 'they', 'few', 'its', 'over', 'how', \"you'll\", \"mightn't\", 'a', 'those', 'nor', 'his', 'ourselves', 'who', 'down', 'haven', 'with', 'does', 'as', 'i', 'to', 'here', 'which', 'until', 'most', 'isn', 'against', 'wouldn', \"doesn't\", 'do', 'such', 'no', 'each', 'if', 'their', \"haven't\", 'mightn', 're', 'above', 'be', 'that', 'myself', 'can', 'theirs', 'wasn', 'up', 'y', 'both', \"it's\", 'too', 'through', 'are', 'him', 'then', 'you', 'at', 'them', 'should', \"didn't\", 'where', 'will', 'don', \"she's\", 'ma', 'it', 'under', \"isn't\", 'only', 'themselves', 'between', 'yourself', 'weren', 'from', 'just', 'very', 't', \"shan't\", 'not', \"hasn't\", 'is', 'doesn', 'your', 'same', 'about', 'before', 'any', 'why', \"needn't\", 'own', 'hasn', 'off', \"you've\", 'on', 've', 'were', 'himself', \"wouldn't\", \"couldn't\", 'been', 'for', 'o', 'our', 'other', 'an', 'but', 'needn', 'being', 'my', 'her', 'hadn', 'shan', 'the', 'what', 'mustn', 'after', 'below', 'd', \"hadn't\", 'all', 'once', \"weren't\", 'whom', 'again', 'further', \"should've\", 'couldn', 'of', 'by', 'has', 'won', \"aren't\", 'than', 'll', 'this', 'so', 'itself', \"wasn't\"}\n"
     ]
    }
   ],
   "source": [
    "# Removing stop wards\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "print('Stop words in English language are: \\n')\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88839b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text after removing stop words \n",
      "\n",
      "['On', 'Wednesday', ',', 'Association', 'Computing', 'Machinery', ',', 'world', '’', 'largest', 'society', 'computing', 'professionals', ',', 'announced', 'Hinton', ',', 'LeCun', 'Bengio', 'year', '’', 'Turing', 'Award', 'work', 'neural', 'networks', '.', 'The', 'Turing', 'Award', ',', 'introduced', '1966', ',', 'often', 'called', 'Nobel', 'Prize', 'computing', ',', 'includes', '$', '1', 'million', 'prize', ',', 'three', 'scientists', 'share', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [w for w in word_tk if not w in sw]\n",
    "print(\"The text after removing stop words \\n\")\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe8413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fd064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
